ğŸ–ï¸ Sign Language Recognition & Text-to-Speech System
A real-time American Sign Language (ASL) recognition system that converts hand gestures into text and speech using Computer Vision and Deep Learning.
Built with Python, OpenCV, TensorFlow, and a custom-trained CNN model, this project empowers accessibility by enabling hands-free communication through sign language.

ğŸš€ Features
âœ… Core Functionalities
Hand Gesture Recognition: Real-time detection using cvzone's HandDetector (21-point landmarks).
Deep Learning Alphabet Prediction: CNN trained on 26 classes (Aâ€“Z) using skeletonized hand images (accuracy >95%).
Text-to-Speech Integration: Converts constructed sentences into audible speech with pyttsx3.
Word & Sentence Suggestions:
Next Word Prediction: NLTK-trained Trigram Language Model on the Brown Corpus.
Auto Spell Correction: SymSpell based on dictionary frequency & edit distance.
Gesture Controls:
âœ‹ Open Hand â†’ Insert Space
ğŸ‘ Thumb Out â†’ Backspace
Hands-Free Operation: No keyboard required â€” control everything with gestures.

ğŸ§  Model Architecture
Input: 55Ã—55 grayscale skeletonized gesture images
Layers:
3Ã— Conv2D + BatchNormalization + MaxPooling + Dropout
Fully connected: 512 & 256 neurons + Dropout
Output: Softmax (26 classes: Aâ€“Z)
Training Techniques:
Data Augmentation (Rotation, Zoom, Shear, Flip)
Early Stopping, ReduceLROnPlateau
ModelCheckpoint for best accuracy

ğŸ“¸ GUI Overview
Built with Tkinter featuring:
Live camera feed
Skeleton drawing panel
Real-time prediction + confidence
Sentence builder with:
Word suggestions
Next word prediction
Control buttons: Speak, Auto-Correct, Backspace, Clear

ğŸ“ File Structure
â”œâ”€â”€ alphabetPred.py         # Skeleton-to-letter prediction
â”œâ”€â”€ final1pred.py           # Full ASL-to-speech GUI system
â”œâ”€â”€ handAcquisition.py      # Data collection tool
â”œâ”€â”€ trainmodel.py           # CNN training script
â”œâ”€â”€ AtoZ_3.1/               # Dataset (26 folders, Aâ€“Z)
â”œâ”€â”€ model-bw.weights.h5     # Trained model weights
â”œâ”€â”€ best_model.h5           # Best validation model
â”œâ”€â”€ model-bw.json           # Model architecture

ğŸ†•Unique Aspects
Skeleton-Based Images: More generalizable & noise-resistant than raw RGB.
Gesture-Based Control: Space & backspace without touching the keyboard.
Context-Aware Suggestions: NLP-powered next word prediction.

ğŸ§ª Future Enhancements
Support for dynamic gestures (motion-based signs)
Expand to numbers & ASL-specific phrases
Web/mobile deployment (Flask / Kivy)

Multi-hand tracking & recognition

ğŸ› ï¸ Technologies Used
Python, OpenCV, TensorFlow/Keras, cvzone, NLTK, SymSpell, Tkinter, Pyttsx3
